---
title: "ST404-Project-1"
author: "group17"
date: "27/01/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(ggplot2)
library(lmtest)
library(grid)
library(corrplot)
```



```{r include=FALSE}
load("USACrime.Rda")
df=USACrime
```

To see which variables are significantly associated with the outcome variables, we will conduct hypothesis tests (see appendix 1.1) on each of the variables. The hypotheses are the following: 


$$H_0: \text{corr}(x_i,y_j)=0, H_1: \text{corr}(x_i,y_j)\neq 0 , i=1,...,20, j=1,2$$

We explain $i$ ranging in the interval $1$ to $20$ due to there being $24$ variables, out of which $2$ are factor variables and $2$ are target variables.  
  
  
We observe that `violentPerPop` and `nonViolPepPop` have very stronger than average correlation $(\gt 0.40)$ with the variables `pctKids2Par` (corr $\in \{-0.73,-0.67\}$ with variables `violentPepPop` and `nonViolPerPop` respectively), `pctKidsBornNevrMarr` (corr $\in \{0.74,0.55\}$), `pctWdiv` (corr $\in \{-.56,-0.49\}$), `pctNotHSgrad` (corr $\in \{0.47,0.37\}$), `pctUnemploy` (corr $\in\{0.47,0.39\}$), `pctHouseOwnerOccup` (corr $\in\{-0.46,-0.46\}$), `pctVacantBoarded` (corr $\in \{0.48,0.32\}$), `medIncome`
(corr $\in\{0.39,0.46\}$).   
  
## Can the relationship be characterized as a linear?

Below you can see the scatterplots of every variable plotted against the dependent variables. 

```{r echo=FALSE, message=FALSE, warning=FALSE,fig.width=10,fig.height=5}

multiplot <- function(..., plotlist=NULL, cols) {
    

    # Make a list from the ... arguments and plotlist
    plots <- c(list(...), plotlist)

    numPlots = length(plots)

    # Make the panel
    plotCols = cols                          # Number of columns of plots
    plotRows = ceiling(numPlots/plotCols) # Number of rows needed, calculated from # of cols

    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(plotRows, plotCols)))
    vplayout <- function(x, y)
        viewport(layout.pos.row = x, layout.pos.col = y)

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
        curRow = ceiling(i/plotCols)
        curCol = (i-1) %% plotCols + 1
        print(plots[[i]], vp = vplayout(curRow, curCol ))
    }
}

df1=df[,c(4,5,7,9,11,12,14,15,23,24)]
plot1=df1 %>%
  gather(-nonViolPerPop, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = nonViolPerPop)) +
    geom_point() +
    facet_wrap(~ var, scales = "free") +
    theme_bw()+geom_smooth()

plot2=df1 %>%
  gather(-violentPerPop, key = "var", value = "value") %>%
  ggplot(aes(x = value, y = violentPerPop)) +
    geom_point() +
    facet_wrap(~ var, scales = "free") +
    theme_bw()+geom_smooth()
multiplot(plot1,plot2,cols=2)
```
From the scatter plots it is evident that the relationships are not linear. However, to confirm this hypothesis, we compare whether a non-linear model would be a better fit through an ANOVA test. Оur results indeed confirm that every variable has a non linear relationship with the dependent variables (see appendix 1.2).
  
## Does the relationship appear to be homoscedastic?

From the scatterplots above we see that the relationships are heteroscedastic. We test our hypothesis with the Breusch-Pagan test and indeed the relationships are hetersocedastic (see appendix 1.3).


## What transformations, if any, might be applied, to resolve any issues?

We applied log tranformations to the dependent variables and this resolved the issue of heteroscedasticity with most of the variables (see appendix 1.4). Before modelling, we recommend log transforming the dependent variables.

## Are there any other approaches that could be taken to tackle these issues?

Other approach include performing the Box-Cox transformation as well fitting a model that accounts for changes in variance.
  
  
  
  
  
  
  
  
  
  
  
  
  
  2) Which variables, if any, have a highly skewed distribution? What transformations might be
  applied to reduce skewness and stabilize the spread of the observations?
  3) Do any of the variables have outlying values? How should outliers be treated?
  4) Which variables are highly correlated with each other? Are there variables that represent
  different ways of measuring the same thing?
  5) Given all of the above, what recommendations would you suggest for preparing these data
  in order to fit a linear model?


TEST CHANGE11







# Correlation between explanatory variables

To investigate the correlation between all explanatory variables, we first plot a correlation matrix with all explanatory variables included (see appendix 2.1).

However, a correlation matrix with 20 variables is difficult to look at. We can remove variables that have low correlation with all other variables, that are variables that have absolute value of correlation coefficient of less than or equal to 0.65 with all other variables, to remove some variables out of the discussion. 

We are then left with the following correlation matrix:

```{r figs, echo=FALSE, fig.width=5,fig.height=4,fig.cap="\\label{fig:figs}Correlation Matrix for selected explanatory variables"}
USACrimeCorrelation <- subset(USACrime,select=-c(State,region,violentPerPop,nonViolPerPop,pctUrban,pctHousOccup,pctHousOwnerOccup,pctVacantBoarded,pctVacant6up,popDensity,pctForeignBorn))
Correlation <- cor(USACrimeCorrelation, use="pairwise.complete.obs")

pvaluematrix <- cor.mtest(USACrimeCorrelation)$p

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(Correlation, method = "color", col = col(200),
         type = "lower", number.cex = 0.5, 
         addCoef.col = "black", tl.cex = 0.6,
         tl.col = "black", tl.srt = 90, 
         p.mat = pvaluematrix, sig.level = 0.01, insig = "blank", 
         diag = FALSE)

```

# Which variables show a strong relationship with the outcome variables?


To see which variables are significantly associated with the outcome variables, we will conduct hypothesis tests on each of the variables. The hypotheses are the following: 


$$H_0: \text{corr}(x_i,y_j)=0, H_1: \text{corr}(x_i,y_j)\neq 0 , i=1,...,20, j=1,2$$

We explain $i$ ranging in the interval $1$ to $20$ due to there being $24$ variables, out of which $2$ are factor variables and $2$ are target variables.

In the following chunk of code we are going to conduct tests with the variable `violentPerPop`:

```{r}
# Subsetting the dataframe so that it excludes State, region and violentPerPop
dfcor1 <- subset(df, select = -c(State, region, violentPerPop,nonViolPerPop))

for(i in colnames(dfcor1)){
  print(i)
  test=cor.test(dfcor1[[i]],df$violentPerPop)
 cat(sprintf("t-statistic is: %s, p-value is: %s, and correlation coefficient estimate is %s",test$statistic, test$p.value,test$estimate))
  writeLines("\n")
}

```

As we can see, the only variable which doesn't have correlation which is significantly different than $0$ is `pctVacant6up`. We also observe very strong correlation with the variables `pctKids2Par` as well as `pctKidsBornNevrMarr`.

We will now examine the relationship between `nonViolPerPop` and the other variables:

```{r}
for(i in colnames(dfcor1)){
  print(i)
  test=cor.test(dfcor1[[i]],df$nonViolPerPop)
  cat(sprintf("t-statistic is: %s, p-value is: %s, and correlation coefficient estimate is %s",test$statistic, test$p.value,test$estimate))
  writeLines("\n")
}
```
The variables `pctUrban` and `pctVacant6upp` don't have correlations which are significantly different than $0$. We also observe `pctKids2Par` and `pctKidsBornNevrMarr` having strong correlation with the respective target.


Having tested on both targets, we can say that the variables `pctKids2Par` and `pctKidsBornNevrMarr` correlate strongly with the targets.

## Can the relationship be characterized as a linear?

We will plot the variables `pctKids2Par`, `pctKidsBornNevrMarr`, `nonViolPerPop` and `violentPerPop` visualizing them in scatterplots:

```{r}
pairs(df[,c(12,11,23,24)])
```

The scatterplots indicate that the `pctKidsBornNevrMarr` and `pctKids2Par` variables may have a linear relationship with each other, although it is not clear whether these variables have linear relationships with the targets.

To see whether we can classify the relationship between these variables as linear, we can fit a linear and nonlinear model and asses whether the nonlinear model explains a significantly larger amount of variance via ANOVA.

We will now test whether the variable `pctKidsBornNevrMarr` performs better with a linear or nonlinear model when modeling for `violentPerPop`:

```{r}
linModel=lm(df$violentPerPop~df$pctKidsBornNevrMarr)
sqModel=lm(df$violentPerPop~poly(df$pctKidsBornNevrMarr,2))

anova(linModel,sqModel)
```

We can see that the p-value$=2.223e-06 \lt 0.001$. So having a non-linear model fitted for the variables `violentPerPop` and `pctKidsBornNevrMarr` did lead to a significantly improved fit over the linear model. 

We now fit the same models for the variables `pctKidsBornNevrMarr` and `nonViolPerPop`:

```{r}
linModel=lm(df$nonViolPerPop~df$pctKidsBornNevrMarr)
sqModel=lm(df$nonViolPerPop~poly(df$pctKidsBornNevrMarr,2))

anova(linModel,sqModel)
```
We again see that the non-linear model leads to a significantly improved fit over the linear model.

We now test for the variables `pctKids2Par` and `nonViolPerPop`:

```{r}
linModel=lm(df$nonViolPerPop~df$pctKids2Par)
sqModel=lm(df$nonViolPerPop~poly(df$pctKids2Par,2))

anova(linModel,sqModel)
```
The result shows a non-significant result, $p=0.6419$. Thus, we fail to accept that the non-linear model provides a better fit and hence the relationship is linear.

And lastly we test the variables `pctKids2Par` and `violentPerPop`:

```{r}
linModel=lm(df$violentPerPop~df$pctKids2Par)
sqModel=lm(df$violentPerPop~poly(df$pctKids2Par,2))

anova(linModel,sqModel)
```
The non-linear model provides a significant improvement in fit.

We can conclude that the only relationship which is close to linear is the relationship between the variables `pctKids2Par` and `nonViolPerPop`. 

## Does the relationship appear to be homoscedastic?

To test whether the  relationships are homoscedastic, we are going to perform the Breusch-Pagan test with the `bptest()` function.

We will test for the variables `pctKidsBornNevrMarr` and `violentPerPop`:



```{r}
install.packages("lmtest")
library(lmtest)
model=lm(violentPerPop~pctKidsBornNevrMarr,data=df)
bptest(model)
```
The p-value is 2.2e-16, hence less than 0.05. This means we have sufficient evidence that there is heteroscedasticity present in the model.

Testing for the variables `pctKidsBornNevrMarr` and `nonViolPerPop`:


```{r}
model=lm(nonViolPerPop~pctKidsBornNevrMarr,data=df)
bptest(model)
```
We have sufficient evidence that there is heteroscedasticity present in the model.

We test for `pctKids2Par` and `violentPerPop`:

```{r}
model=lm(violentPerPop~pctKids2Par,data=df)
bptest(model)
```
We have sufficient evidence that there is heteroscedasticity present in the model.

We test for `pctKids2Par` and `nonViolPerPop`:

```{r}
model=lm(nonViolPerPop~pctKids2Par,data=df)
bptest(model)
```
We have sufficient evidence that there is heteroscedasticity present in the model.


In all of the relationships that we have examined for presence of homoscedasticity, we have found homoscedasticity present in every relationship. 


<<<<<<< HEAD
We have sufficient evidence that there is heteroscedasticity present in the model.

In all of the relationships that we have examined for presence of homoscedasticity, we have found heteroscedasticity present in every relationship. 

# 1.4

### What transformations, if any, might be applied, to resolve any issues? 
=======
# What transformations, if any, might be applied, to resolve any issues? 
>>>>>>> c7e77871296b28678eca0e88c280e9a8cdf1cf2e

If heteroscedasticity is the issue we can apply a log transformation on the targets.


<<<<<<< HEAD
```{r}
model=lm(log(violentPerPop)~pctKidsBornNevrMarr,data=df)
bptest(model)
```
=======
# Are there any other approaches that could be taken to tackle these issues?
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  2) Which variables, if any, have a highly skewed distribution? What transformations might be
  applied to reduce skewness and stabilize the spread of the observations?
  3) Do any of the variables have outlying values? How should outliers be treated?
## Question: Do any of the variables have outlying values? How should outliers be treated?
>>>>>>> c7e77871296b28678eca0e88c280e9a8cdf1cf2e

The outlying values are unusually the large or small values that fall beyond the overall pattern.

To look at the outliers of the variables, we will choose function "boxplots" and focus on the points lying outside the box pattern.

For example, we first draw the boxplot of the variable "pctNotHSgrad".

```{r echo=FALSE}
library(ggplot2)
ggplot(df, aes( y=pctNotHSgrad)) + 
geom_boxplot(aes(fill = "red"))
```

After drawing the boxplot, we observe that there are about 14 dark points fell beyond the overall pink pattern. 

From the boxplots of all univariate variables, it is clear that varirables "medIncome", "pctWdiv", "pctLowEdu", "pctNotHSgrad", "pctCollGrad", "pctUnemploy", "pctEmploy", "pctKids2Par", "pctKidsBornNevrMarr", "pctHousOccup", "pctHousOwnerOccup", "pctVacantBoarded", "pctVacant6up", "ownHousMed ", "ownHousQrange", "rentMed", "rentQrange", "popDensity", "pctForeignBorn", "violentPerPop" and "nonViolPerPop" have outlying values.


```{r echo=FALSE, fig.width=7}
ggplot(df, aes(x = State, y = violentPerPop ))+ 
geom_boxplot(aes(fill = State))
```

We could have a look at the outlying values of the violence per population in every state. Here it is very clear that all outlying values are appeared as dark points out of the colourful patterns.

Another example of outlying values in bivariate variables plot.

```{r echo=FALSE}
x <- lm(df$violentPerPop~ df$pctNotHSgrad)
plot(df$violentPerPop~ df$pctNotHSgrad ,main="violentPerPop versus pctNotHSgrad",col="blue")
abline(x, col = "red")
```

We will particularly look at points with large distance from the red line.

We will check for a data entry error firstly and then examine the physical context.
We might need to pay attention to these outlying values when we fit a model: exclude the point from the analysis but try including it later to see if the model is changed. At this stage, we prefer to keep these outlying values. A single point won’t have the leverage to affect the fit very much. It’s still worth identifying outliers if these type of points are worth knowing about in the particular application. We only need to care about cluster of outliers which may hide each other.
  
  
  4) Which variables are highly correlated with each other? Are there variables that represent
  different ways of measuring the same thing?
  5) Given all of the above, what recommendations would you suggest for preparing these data
  in order to fit a linear model?


TEST CHANGE11







# Correlation between explanatory variables

To investigate the correlation between all explanatory variables, we first plot a correlation matrix with all explanatory variables included (In Appendix).

However, a correlation matrix with 20 variables is difficult to look at. We can remove variables that have low correlation with all other variables, that are variables that have absolute value of correlation coefficient of less than or equal to 0.65 with all other variables, to remove some variables out of the discussion. 

We are then left with the following correlation matrix:

```{r figs, echo=FALSE, fig.width=5,fig.height=4,fig.cap="\\label{fig:figs}Correlation Matrix for selected explanatory variables"}
USACrimeCorrelation <- subset(USACrime,select=-c(State,region,violentPerPop,nonViolPerPop,pctUrban,pctHousOccup,pctHousOwnerOccup,pctVacantBoarded,pctVacant6up,popDensity,pctForeignBorn))
Correlation <- cor(USACrimeCorrelation, use="pairwise.complete.obs")

pvaluematrix <- cor.mtest(USACrimeCorrelation)$p

col <- colorRampPalette(c("#BB4444", "#EE9988", "#FFFFFF", "#77AADD", "#4477AA"))
corrplot(Correlation, method = "color", col = col(200),
         type = "lower", number.cex = 0.5, 
         addCoef.col = "black", tl.cex = 0.6,
         tl.col = "black", tl.srt = 90, 
         p.mat = pvaluematrix, sig.level = 0.01, insig = "blank", 
         diag = FALSE)
```

Using this correlation matrix, we are able to identify pairs or clusters of explanatory variables that are highly correlated to each other. Then we can pick one that best represents the pair/cluster and remove the rest during model building. During this part of the investigation, we would assume that two variables are highly correlated if the absolute value for their correlation coefficient is larger than 0.65.

1. The first pair of highly correlated variables is 
  + **`medIncome`**, median household income
  + **`pctWdiv`**, percentage of households with investment/rent income
  
|       Both variables measures how wealthy people are in the community. Hence, we recommend using **`medIncome`** as the only wealth measuring variable.
\

2. The second cluster of highly correlated variables is
  + **`pctLowEdu`**, percentage of people 25 and over with less than a 9th grade education
  + **`pctNotHSgrad`**, percentage of people 25 and over that are not high school graduates
  + **`pctCollGrad`**,percentage of people 25 and over with a bachelors degree or higher education
  
|       All three variables measures the education level of the people in the community. Hence, we recommend using **`pctLowEdu`** as the only education measuring variable.
\

3. The third pair of highly correlated variables is
  + **`pctUnemployed`**, percentage of people 16 and over, in the labor force, and unemployed
  + **`pctEmploy`**, percentage of people 16 and over who are employed

|       Both variables measures the employability of the people in the community. Hence, we recommend using **`pctUnemployed`** as the only employment measuring variable.
\


4. The forth pair of highly correlated variables is 
  + **`pctKids2Par`**, percentage of kids in family housing with two parents
  + **`pctKidsBornNevrMarr`**, percentage of kids born to never married
  
|       Both variables measures the completeness of family of the people in the community. Hence, we recommend using **`pctKids2Par`** as the only completeness of family measuring variable.
\

5. The last cluster of highly correlated variables is
  + **`ownHouseMed`**, owner occupied housing - median value
  + **`ownHousQrange`**, owner occupied housing - difference between upper quartile and lower quartile values
  + **`rentMed`**, rental housing - median rent

|       All three variables measures the cost of living(housing) for the people in the community. Hence, we recommend using **`ownHouseMed`** as the only completeness of family measuring variable.

In the end, we recommend using these remaining variables for model building purposes.








# Appendix

```{r echo=FALSE}
USACrimeCorrelation <- subset(USACrime, select=-c(State,region,violentPerPop,nonViolPerPop))
Correlation <- cor(USACrimeCorrelation, use="pairwise.complete.obs")

corrplot(Correlation, type="lower", method="number", number.cex=0.5, tl.col="black")
```
